<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>神经网络 | TAY&#39;S BLOG</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="神经元模型神经网络的定义：神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应 神经元：神经网络中最基本的成分是神经元 (neuron) 模型，即上述定义中的“简单单元” M-P神经元模型​	这个模型中，神经元接收到来自几个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络">
<meta property="og:url" content="http://example.com/2024/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="TAY&#39;S BLOG">
<meta property="og:description" content="神经元模型神经网络的定义：神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应 神经元：神经网络中最基本的成分是神经元 (neuron) 模型，即上述定义中的“简单单元” M-P神经元模型​	这个模型中，神经元接收到来自几个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比">
<meta property="og:locale" content="zh_CH">
<meta property="article:published_time" content="2024-05-31T12:19:17.000Z">
<meta property="article:modified_time" content="2024-06-04T07:50:21.888Z">
<meta property="article:author" content="tay">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="TAY'S BLOG" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">TAY&#39;S BLOG</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Suche"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-神经网络" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="article-date">
  <time class="dt-published" datetime="2024-05-31T12:19:17.000Z" itemprop="datePublished">2024-05-31</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      神经网络
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h1><p><strong>神经网络的定义</strong>：神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互反应</p>
<p><strong>神经元</strong>：神经网络中最基本的成分是神经元 (neuron) 模型，即上述定义中的“简单单元”</p>
<h2 id="M-P神经元模型"><a href="#M-P神经元模型" class="headerlink" title="M-P神经元模型"></a>M-P神经元模型</h2><p>​	这个模型中，神经元接收到来自几个其他神经元传递过来的输入信号，这些输入信号通过带<strong>权重</strong>的连接进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过“激活函数”处理以产生神经元的输出。</p>
<p>​	理想的激活函数为阶跃函数，它将输入值映射为0或1.但它具有不连续，不光滑等不太好的性质，因此常使用Sigmoid函数作为激活函数。</p>
<h1 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h1><p>​	感知机由两层神经元组成（输入层，输出层），输入层接受外界输入信号后传递给输出层，输出层是M-P神经元（亦称“阈值逻辑单元”）。感知机能容易地实现逻辑与，或，非运算。</p>
<p>​	更一般地，给定训练数据集,权重 wi ( i&#x3D;1,2,…, n) 以及阈值 xita 可通过学习得到.阈值。可看作一个固定输入为-1.0的“哑结点”(dummy node)所对 应的连接权重w(n+1)这样，权重和阈值的学习就可统一为权重的学习.</p>
<p>​	调整权重：deta（wi）&#x3D;n（y-y‘）xi，wi更新为wi+deta（wi），其中n的范围为（0，1），成为学习率（一般取0.1</p>
<p>​	当预测正确时，y&#x3D;y’，感知机不发生变化，否则将根据错误的程度进行权重调整.</p>
<p>​	但对于只有两层神经元的感知机，它只能处理线性可分的问题，而无法解决非线性可分的问题，因此要考虑使用<strong>多层功能神经元</strong></p>
<p><strong>隐层或隐含层</strong>：位于输出层与输入层之间的一层拥有激活函数的功能神经元</p>
<p><strong>多层前馈神经网络</strong>：每一层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接。其中输入层神经元接受外界输入，隐层和输出层神经元对信号进行加工，最终结果由输出层神经元输出。</p>
<p><strong>神经网络“学”到的东西，蕴含在连接权与阈值中</strong></p>
<h1 id="误差逆传播算法"><a href="#误差逆传播算法" class="headerlink" title="误差逆传播算法"></a>误差逆传播算法</h1><p>​	每个训练样例， BP 算法(误差逆传播算法)执行以下操作：先将输入示 例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果；然后计算输出层的误差，再将误差逆向传播至隐层神经元,最后根据隐层神经元的误差来对连接权和阈值进行调整. 该迭代过程循环进行，直到达到某些停止条件为止，例如训练误差已达到一个很小的值 。</p>
<p>累计误差逆传播算法：类似bp算法，但是基于累计误差最小的更新规则，以降低参数更新频率</p>
<h2 id="缓解过拟合"><a href="#缓解过拟合" class="headerlink" title="缓解过拟合"></a>缓解过拟合</h2><p>​	由于强大的表示能力，bp神经网络常遭遇过拟合</p>
<p><strong>早停</strong>：将数据分成训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值.</p>
<p><strong>正则化</strong>：其基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分</p>
<h1 id="全局最小与局部极小"><a href="#全局最小与局部极小" class="headerlink" title="全局最小与局部极小"></a>全局最小与局部极小</h1><p>​	局部极小解是参数空间中的某个点，其邻域点的误差函数值均不小于该点的函数值;全局最小解则是指参数空间中所有点的误差函数值均不小于该点的误差函数值。在参数空间中可能存在多个局部极小值，但却只会有一个全局最小值</p>
<p>​	“跳出”局部极小，逼近全局最小：</p>
<p>1.以多组不同参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数.这相当于从多个不同的初始点开始搜索，这样就可能陷入不同的局部极小，从中进行选择有可能获得更接近全局最小的结果.</p>
<p>2.使用“模拟退火” 技术 ，模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助 于“跳出”局部极小在每步迭代过程中，接受“次优解”的概率要随着 时间的推移而逐渐降低，从而保证算法稳定.</p>
<p>3.使用随机梯度下降.与标准梯度下降法精确计算梯度不同，随机梯度下降 法在计算梯度时加入了随机因素.于是，即便陷入局部极小点，它计算出 的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索.</p>
<p>4.遗传算法</p>
<h1 id="其它常见神经网络"><a href="#其它常见神经网络" class="headerlink" title="其它常见神经网络"></a>其它常见神经网络</h1><h2 id="PBR网络"><a href="#PBR网络" class="headerlink" title="PBR网络"></a>PBR网络</h2><p>​	它使用径向基函数作为隐层神经元激活函数，而输出层则是对隐层神经元输出的线性组合.</p>
<h2 id="ART网络"><a href="#ART网络" class="headerlink" title="ART网络"></a>ART网络</h2><p>​	<strong>竞争型学习</strong>是神经网络中一种常用的无监督学习策略，在使用该策略时，网络的输出神经元相互竞争，每一时刻仅有一个竞争获胜的神经元被激活，其他神经元的状态被抑制.这种机制亦称“胜者通吃”原则.</p>
<p>​	ART网络由比较层，识别层，识别阈值，重置模块构成。其中比较层负责接收输入样本，并将其传递给识别层神经元。识别层每一个神经元对应一个模式类，神经元数目可在训练过程中动态增长以增加新的模式类</p>
<p>​	在接收到比较层的输入信号后，识别层神经元之间相互竞争以产生获胜神经元.竞争的最简单方式是，计算输入向量与每个识别层神经元所对应的模式 类的代表向量之间的距离，距离最小者胜.获胜神经元将向其他识别层神经元发送信号，抑制其激活.若输入向量与获胜神经元所对应的代表向量之间的相似度大于识别阈值，则当前输入样本将被归为该代表向量所属类别，同时，网络 连接权将会更新，使得以后在接收到相似输入样本时该模式类会计算出更大的 相似度，从而使该获胜神经元有更大可能获胜；若相似度不大于识别阈值,则重 置模块将在识别层增设一个新的神经元,其代表向量就设置为当前输入向量.</p>
<h2 id="SOM网络"><a href="#SOM网络" class="headerlink" title="SOM网络"></a>SOM网络</h2><p>​	SOM网络是一种竞争学习型的无监督神经网络，它能将高维输入数据映射到低维空间，同时保持输入数据在高维空间的拓扑结构</p>
<p>​	SOM 的训练过程很简单：在接收到一个训练样本后，每个输出层神经元会计算该样本与自身携带的权向量之间的距离，距离最近的神经元成为竞争获胜者，称为最佳匹配单元(best matching unit). 然后，最佳匹配单元及其邻近神经元的权向量将被调整，以使得这些权向量与当前输入样本的距离缩小.这个过程不断迭代，直至收敛.</p>
<h2 id="级联相关网络"><a href="#级联相关网络" class="headerlink" title="级联相关网络"></a>级联相关网络</h2><p>​	结构自适应网络将网络结构也作为学习的目标之一，并希望能在训练过程中找到最符合数据特点的网络结构。</p>
<p>​	级联相关网络有两个主要成分：“级联”和“相关”.级联是指建立层次连接的层级结构.在开始训练时，网络只有输入层和输出层，处于最小拓扑结 构；随着训练的进行，新的隐层神经元逐渐加入，从而创建起层级结构.当新的隐层神经元加入时，其输入端连接权值是冻结固定的.相关是指通过最大化新神经元的输出与网络误差之间的相关性来训练相关的参数.</p>
<h2 id="Elman网络"><a href="#Elman网络" class="headerlink" title="Elman网络"></a>Elman网络</h2><p>​	“递归神经网络”允许网络中出现环形结构，从而可让一些神经元的输出反馈回来作为输入信号。这使得网络在 t 时刻的输出状态不仅与 t 时刻的输入有关，还与 t-1 时刻的网络状态有关，从而能处理与时间有关的动态变化。</p>
<p>​	Elman网络时最常见的递归神经网络之一。它的结构与多层前馈网络很相似,但隐层神经元的输出被反馈回来，与下 一时刻输入层神经元提供的信号一起，作为隐层神经元在下一时刻的输入.隐层神经元通常采用 Sigmoid 激活函数，而网络的训练则常通过推广的 BP 算法进行</p>
<h2 id="Boltzmann机"><a href="#Boltzmann机" class="headerlink" title="Boltzmann机"></a>Boltzmann机</h2><p>​	神经网络中有一类模型是为网络状态定义一个“能量” , 能量最小化时网络达到理想状态，而网络的训练就是在最小化这个能量函数. Boltzmann 机就是一种“基于能量的模型” 。其神经元分为两层：显层与隐层.显层用 于表示数据的输入与输出，隐层则被理解为数据的内在表达. Boltzmann 机中的神经元都是布尔型的，即只能取 0 或 1 两种状态，状态 1 表示激活，状态 0 表 示抑制.</p>
<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><p>​	随着计算能力的大幅提高，我们可以从单纯的增加隐层神经元的数目，变为增加隐层的数目，更为有效。</p>
<h2 id="无监督逐层训练"><a href="#无监督逐层训练" class="headerlink" title="无监督逐层训练"></a>无监督逐层训练</h2><p>​	基本思想是每一次训练一层隐结点，训练时将上一层隐结点的输出作为输入，而本层隐结点的输出作为下一层隐结点的输入，称为“预训练”。在预训练全部完成后，再对整个网络进行“微调”。</p>
<p>​	事实上，“预训练+微调”的做法可视为将大量参数分组,对每组先找到局部看来比较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优. 这样就在利用了模型大量参数所提供的自由度的同时,有效地节省了训练开销.</p>
<h2 id="权共享"><a href="#权共享" class="headerlink" title="权共享"></a>权共享</h2><p>​	即让一组神经元使用相同的连接权。</p>
<p>​	CNN 复合 多个“卷积层”和“采样层”对输入信号进行加工，然后-在连接层实现与输出目标之间的映射.每个卷积层都包 含多个特征映射 , 每个特征映 射是一个由多个神经元构成的“平面”，通过一种卷积滤波器提取输入的一种特征.样层亦称为“汇合” 层，其作用是基于局部相关性原理进行亚采样，从而在减少数据量的同时保留有用信息.无论是卷积层还是采样层，其每一组神经元都是 用相同的连接权，从而大幅减少了需要训练的参数数目.</p>
<p>​	论是 DBN 还是 CNN, 其多隐层堆叠、每层对上一层的输出进行处理的机制，可看作是在对输入信号进行逐层加工，从而把初始的、与输出目标之间联系不太密切的输入表示，转化成与输出目标联系更密切的表示，使得原来仅基于最后一层输出映射难以完成的任务成为可能.换言之，通过多层处理，逐渐将初始的“低层”特征表示 转化为“高层”特征表示后，用“简单模型”即可完成复杂的分类等学习任务.由此可将深度学习理解为进行“特征学习” 或“表示学 习 ”.</p>
<p>​	</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/05/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" data-id="cm2vmjbad004dwwtgeeqpaijk" data-title="神经网络" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/06/04/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          贝叶斯分类器
        
      </div>
    </a>
  
  
    <a href="/2024/05/30/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">线性模型</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/DataStructure/" rel="tag">DataStructure</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm-template/" rel="tag">algorithm-template</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/attacks/" rel="tag">attacks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ctf/" rel="tag">ctf</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ctf-misc/" rel="tag">ctf-misc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/encryption/" rel="tag">encryption</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/learning/" rel="tag">learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine/" rel="tag">machine</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning-attacks/" rel="tag">machine learning attacks</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tip/" rel="tag">tip</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E4%BF%9D%E7%A0%94/" rel="tag">保研</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%A4%8D%E4%B9%A0/" rel="tag">复习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E5%B0%8F%E7%BB%93/" rel="tag">每日一小结</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/DataStructure/" style="font-size: 10px;">DataStructure</a> <a href="/tags/algorithm-template/" style="font-size: 10px;">algorithm-template</a> <a href="/tags/attacks/" style="font-size: 13.33px;">attacks</a> <a href="/tags/ctf/" style="font-size: 20px;">ctf</a> <a href="/tags/ctf-misc/" style="font-size: 13.33px;">ctf-misc</a> <a href="/tags/encryption/" style="font-size: 13.33px;">encryption</a> <a href="/tags/learning/" style="font-size: 15px;">learning</a> <a href="/tags/machine/" style="font-size: 15px;">machine</a> <a href="/tags/machine-learning/" style="font-size: 16.67px;">machine learning</a> <a href="/tags/machine-learning-attacks/" style="font-size: 16.67px;">machine learning attacks</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/tip/" style="font-size: 11.67px;">tip</a> <a href="/tags/%E4%BF%9D%E7%A0%94/" style="font-size: 10px;">保研</a> <a href="/tags/%E5%A4%8D%E4%B9%A0/" style="font-size: 13.33px;">复习</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E5%B0%8F%E7%BB%93/" style="font-size: 18.33px;">每日一小结</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">March 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">February 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/01/">January 2025</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">November 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/09/">September 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/05/">May 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">April 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/03/10/MPAF/">MPAF</a>
          </li>
        
          <li>
            <a href="/2025/02/20/Poisoning-Machine-Learning-Models-to-Reveal-Their-Secrets/">Poisoning Machine Learning Models to Reveal Their Secrets</a>
          </li>
        
          <li>
            <a href="/2025/01/22/%E6%95%B0%E6%A8%A1-1/">数模</a>
          </li>
        
          <li>
            <a href="/2025/01/08/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/">编译原理</a>
          </li>
        
          <li>
            <a href="/2024/12/31/ds%E7%AC%94%E8%AE%B0/">ds笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 tay<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>